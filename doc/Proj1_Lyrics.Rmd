---
title: "Project 1, Applied Data Science"
author: "Tushar Ponkshe"
date: 'Due: 9/18/2019'
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE, warning=FALSE,echo=FALSE}
packages.used=c("tm", "tidytext","tidyverse","DT","wordcloud","scales","gridExtra","ngram","igraph","ggraph","rsconnect")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
#most of the libraries needed
library(dplyr) #data manipulation
library(textdata)
library(ggplot2) #visualizations
library(gridExtra) #viewing multiple plots together
library(tidytext) #text mining
library(wordcloud2) #creative visualizations
library(knitr) # for dynamic reporting
library(kableExtra) # create a nicely formated HTML table
library(formattable) # for the color_tile function
library(stringr)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(scales)
library(cowplot)
```

## Introduction

The objective of this project was to analyze trends in music lyrics over 4 decades of collected lyrics data. The entire dataset - "lyrics.csv" is a filtered corpus of 380,000+ song lyrics from from MetroLyrics. You can read more about it on [Kaggle](https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics). For the purpose of this report I have used a cleaned version of lyrics data provided to us in starter code.

The starter code in Text_Processing.Rmd cleans the original dataset by removing stop words and performing various preprocessing tasks such as converting all the lyrics to the lower case, removing punctuation, numbers, empty words, extra white spaces, and stemming. The cleaned version of data is then saved as a RData file. 


```{r, message=FALSE, warning=FALSE,echo=FALSE}
#loading processed lyrics data
load('../output/processed_lyrics.RData')
artists = read_csv('../data/artists.csv')
names(artists) = c("artist", "intro", "formed", "members", "origin")
leftJoinDf = dt_lyrics %>%
  left_join(artists, by = "artist")
```



## EDA


First, I read the preprocessed csv file "processed_lyrics.RData". We can see that the dataset contains 125,704 rows and 11 columns or features. 

```{r,warning=FALSE, message=FALSE,echo=FALSE}
# # observations and columns
dim(leftJoinDf)

```


We can also check the structure of the stemmed lyrics - 

```{r,warning=FALSE, message=FALSE,echo=FALSE}
#structure of the lyrics
str(leftJoinDf[1, ]$stemmedwords, nchar.max = 300)
```

We can see that the lyrics have been successfully stemmed and don't contain any stop words.


```{r,warning=FALSE, message=FALSE,echo=FALSE}
#Creating buckets for decades
leftJoinDf <- leftJoinDf %>%
  mutate(decade = 
           ifelse(year %in% 1970:1980, "1970s",
           ifelse(year %in% 1980:1989, "1980s", 
           ifelse(year %in% 1990:1999, "1990s", 
           ifelse(year %in% 2000:2009, "2000s", 
           ifelse(year %in% 2010:2020, "2010s", 
                  "NA"))))))
```


```{r,warning=FALSE, message=FALSE,echo=FALSE}
theme_lyrics <- function() 
{
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_blank(), 
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none")
}
```


##### What's the distribution of number of songs over 5 decades?

One of the target questions is to look for song trends across time, and the dataset contains individual release years - we can create buckets and group the years into decades. The plot below shows number of songs released in each decade starting from 1960s to 2010s. 

```{r,warning=FALSE, message=FALSE,echo=FALSE}
library(magrittr)
leftJoinDf %>%
  filter(decade != "NA", genre != "Other", genre != "Not Available") %>%
  dplyr::group_by(decade, genre) %>%
  dplyr::summarise(number_of_songs = n()) %>%
  ggplot() + 
  geom_bar(aes(x = decade, y = number_of_songs, 
               fill = genre), stat = "identity")  +
  theme(plot.title = element_text(hjust = 0.5),
        legend.title = element_blank(),
        panel.grid.minor = element_blank()) +
  ggtitle("Released Songs") +
  labs(x = NULL, y = "Song Count")
```

The plot above shows that the most active decade was 2000s, with Rock being the most popular genre.


$~$

#### What are the songs with most number words?

```{r,warning=FALSE, message=FALSE,echo=FALSE}
library(magrittr)
full_word_count <- leftJoinDf %>%
  unnest_tokens(word, stemmedwords) %>%
  dplyr::group_by(song, artist, genre) %>%
  dplyr::summarise(num_words = n()) %>%
  arrange(desc(num_words)) 

full_word_count[1:10,] %>%
  ungroup(num_words, song)
```

It's interesting to see that songs in top 5 are mostly hip-hop. This is an expected result because rap songs are often categorized as hip-hop.

#### What is the distribution of word count across genres?

```{r,warning=FALSE, message=FALSE,echo=FALSE}

plt1 = full_word_count %>%
  filter(genre != "Other", genre != "Not Available", num_words <= 500) %>%
  ggplot() +
    geom_histogram(aes(x = num_words, fill = genre )) +
    ylab("Song Count") + 
    xlab("Number of Words") +
    ggtitle("Word Count Distribution, num_words <= 500") +
    theme(plot.title = element_text(hjust = 0.5),
          legend.title = element_blank(),
          panel.grid.minor.y = element_blank())

plt2 = full_word_count %>%
  filter(genre != "Other", genre != "Not Available", num_words > 500) %>%
  ggplot() +
    geom_histogram(aes(x = num_words, fill = genre )) +
    ylab("Song Count") + 
    xlab("Number of Words") +
    ggtitle("Word Count Distribution, num_words > 500") +
    theme(plot.title = element_text(hjust = 0.5),
          legend.title = element_blank(),
          panel.grid.minor.y = element_blank())

plt1
plt2
```

The first histogram of songs with fewer than 500 words is very skewed to the right, meaning that most songs tend to have 0-200 words. 

The second histogram, also skewed to the right, shows that very few songs have more than 500 words. The ones that do are mostly hip-hop, given its rap style.


$~$

#### What are the most frequently used words?

```{r,warning=FALSE, message=FALSE,echo=FALSE}
undesirable_words = c("lot", "today", "months", "month", "wanna", "wouldnt", "wasnt", "ha", "na", "ooh", "da",
        "gonna", "im", "dont", "aint", "wont", "yeah", "la", "oi", "nigga", "fuck",
          "hey", "year", "years", "last", "past", "feel")
song_words_filtered = leftJoinDf %>%
  unnest_tokens(word, stemmedwords) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(nchar(word) > 3)

song_words_filtered %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
    geom_col(aes(word, n), fill = "#0072B2") +
    theme(legend.position = "none", 
          plot.title = element_text(hjust = 0.5),
          panel.grid.major = element_blank()) +
    xlab("") + 
    ylab("Song Count") +
    ggtitle("Most Frequently Used Words") +
    coord_flip()
```

The above figure shows the top 10 most common words in the lyrics dataset. 

$~$

#### Word Cloud #1: Most frequently used words in corpus**

```{r,warning=FALSE, message=FALSE,echo=FALSE}
word_counts <- song_words_filtered %>%
  dplyr::count(word, sort = TRUE)

set.seed(1234)
wordcloud(words = word_counts$word, freq = word_counts$n, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(9, "Dark2"))
```

Words that are most frequently used are in the center and have bigger fonts. Each color represents groups of word that have similar frequencies. Frequency decreases as we move away from the center.


#### What are the most frequent words in each genre?

```{r,warning=FALSE, message=FALSE,echo=FALSE}
popular_words <- song_words_filtered %>% 
  filter(genre != "Not Available", genre != "Other") %>%
  group_by(genre) %>%
  dplyr::count(word, genre, sort = TRUE) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(genre,n) %>%
  dplyr::mutate(row = row_number()) 

popular_words %>%
  ggplot(aes(row, n, fill = genre)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "Song Count") +
    ggtitle("Popular Words by Genre") + 
    facet_wrap(~genre, scales = "free") +
    scale_x_continuous(  # This handles replacement of row 
      breaks = popular_words$row, # notice need to reuse data frame
      labels = popular_words$word) +
    coord_flip()
```

We see that all genres have common words like love, time, heart, and baby.


$~$

#### What are the most frequent words in each decade?


```{r, message=FALSE, warning=FALSE,echo=FALSE}
ts <- song_words_filtered %>% 
  filter(decade != 'NA') %>%
  group_by(decade) %>%
  dplyr::count(word, decade, sort = TRUE) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(decade,n) %>%
  dplyr::mutate(row = row_number()) 

ts %>%
  ggplot(aes(row, n, fill = decade)) +
    geom_col() +
    labs(x = NULL, y = "Song Count") +
    ggtitle("Top words across decades") + 
    theme_lyrics() +  
    facet_wrap(~decade, scales = "free", ncol = 5) +
    scale_x_continuous(  # This handles replacement of row 
      breaks = ts$row, # notice need to reuse data frame
      labels = ts$word) +
    coord_flip()
```


We see a similar trend -- words like love, time, and heart appear in all decades, with order slightly changed.

$~$

#### What is the length of individual words?

We can also see length of individual words used in songs

```{r,warning=FALSE, message=FALSE,echo=FALSE}
word_lengths <- leftJoinDf %>%
  unnest_tokens(word, stemmedwords) %>%
  group_by(song,decade) %>%
  distinct() %>%
  mutate(word_length = nchar(word)) 

word_lengths %>%
  dplyr::count(word_length, sort = TRUE) %>%
  ggplot(aes(word_length), 
         binwidth = 10) + 
    geom_histogram(aes(fill = ..count..),
                   breaks = seq(1,25, by = 2), 
                   show.legend = FALSE) + 
    xlab("Word Length") + 
    ylab("Word Count") +
    ggtitle("Word Length Distribution") +
    theme(plot.title = element_text(hjust = 0.5),
          panel.grid.minor = element_blank())
```

We observe that the histogram is skewed to the right, meaning that most words are less than 15 letters. This makes sense because longer words are not very common and are very hard to rhyme. 

It's interesting to see what these long words are through a wordcloud -->

#### Word Cloud #2: What are these really long words?**

```{r,warning=FALSE, message=FALSE,echo=FALSE}
wc <- word_lengths %>%
  ungroup() %>%
  dplyr::select(word, word_length) %>%
  distinct() %>%
  arrange(desc(word_length))

wordcloud2(wc[1:100, ], 
           size = .25,
           minSize = .0005,
           ellipticity = .3, 
           rotateRatio = 1, 
           fontWeight = "bold")
```

Most of these words contain repeated letters; these words may have been used to complement tune and rythm; not so much for conveying meaning. Also, some of these words have resulted from combining multiple words which also means that they're not for conveying meaning.

$~$

```{r,warning=FALSE, message=FALSE,echo=FALSE}
tidy_reviews <- leftJoinDf %>% 
  unnest_tokens(word, stemmedwords) %>%
  group_by(id) %>% 
  dplyr::mutate(position_in_review_0 = 1:n())
  
cleaned_reviews <- tidy_reviews

```


#### Word Cloud #3 - top words grouped by emotions

```{r,warning=FALSE, message=FALSE,echo=FALSE}
library(reshape2)
library(wordcloud)
cleaned_reviews %>%
  inner_join(get_sentiments("nrc"), by = "word") %>% 
  dplyr::count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0, fun.aggregate = length) %>% 
  comparison.cloud(max.words = 200, title.size = 0.5, scale=c(0.5,1))
```


#### What are some positive and negative words in the corpus according to bing dictionary?

```{r,warning=FALSE, message=FALSE,echo=FALSE}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

cleaned_reviews %>%
  inner_join(nrc_joy) %>%
  dplyr::count(word, sort = TRUE)

bing_word_counts <- cleaned_reviews %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts[21:31,]
```

The dictionary correctly categorized most words in the corpus. Words like "free" and "love" are positive, while the words "kill", "lie" are negative.



#### Highest tf-idf words, grouped by genre

Term-frequency-inverse document frequency (TF-IDF) is another way to judge the topic of an article by the words it contains. With TF-IDF, words are given weight â€“ TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.

In the figure below, we see words that are most relevant to the genres, instead of words most frequent within genres.

```{r,warning=FALSE, message=FALSE,echo=FALSE}
lyrics_words <- leftJoinDf %>%
  filter(genre != "Other", genre != "Not Available") %>%
  unnest_tokens(word, stemmedwords) %>%
  dplyr::count(genre, word, sort = TRUE)

total_words <- lyrics_words %>% 
  group_by(genre) %>% 
  dplyr::summarize(total = sum(n))

lyrics_words <- left_join(lyrics_words, total_words)

lyrics_words <- lyrics_words %>%
  bind_tf_idf(word, genre, n)

lyrics_words %>%
  arrange(desc(tf_idf)) %>%
  dplyr::mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(genre) %>% 
  top_n(10) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = genre)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  theme_lyrics()+
  facet_wrap(~genre, scales = "free") +
  coord_flip()
```

## Conclusion

**In this project report I have explored various trends in the lyrics data. Some of the topics I have explored are listed below:**

- EDA

- Distribution of number of songs over 5 decades

- Songs with most number of words

- Distribution of word count across genres

- Most frequently used words in coupus (with wordcloud)

- Most frequent words in each genre

- Most frequent words in each decade

- Length of individual words and identifying really long words (wordcloud)

- Words groupd by emotions (wordcloud)

- Identifying some positive/negative words in corpus according to bing dictionary

- TF-IDF words across genres

**Limitations and Future Directions**

- Wordcloud #2 suggests possible issues with data entry

- In the future I hope to do an in-depth analysis of artists in each genre over 5 decades

- The artists.csv file contains artists's locations, origin etc which can be used for further analysis


## Resources used

- https://towardsdatascience.com/text-analysis-of-successful-song-lyrics-e41a4ccb26f5

- https://www.datacamp.com/community/tutorials/R-nlp-machine-learning

- https://www.tidytextmining.com/topicmodeling.html
